{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1) What is Spark?__\n",
    "-  Apache Spark is an open source parallel processing framework for running large-scale data analytics applications across clustered computers. It can handle both batch and real-time analytics and data processing workloads.\n",
    "\n",
    "__2) Spark Vs Hadoop__\n",
    "-  Both Hadoop and Spark uses Map Reduce, but the Sparks imlementation of Map Reduce is different from Hadoop.\n",
    "-  __How Hadoop handles MapReduce:__ Hadoop Map Reduce stores the intermediate outputs from the no.of mappers to disk i.e for example lets say there are 10 mappers then there will be 10 __Physical Write__ operations to HDD from these 10 mappers to store the outputs from these mappers then there will be 10 __Physical Read__ Operations from Reduce Phase, this makes Hadoop Very slow when compared to Spark, Consider if there is a __Iterative Algorithm__ like __Logistic Regression__ and lets say there are 100 Iterations and lets say there are 10 mappers then there will be 100*10(Physical Write Operations) + 100*10(Physical Read Operations) = 2000 Physical I/O Operations, this makes very slow execution. \n",
    "-  __How Spark Handles MapReduce:__ Spark also uses MapReduce similar to Hadoop, but the key difference here is, Spark doesn't store the Intermediate Outputs to HDD, Instead it __Caches__ the Intermediate outputs in __RAM__(This is called  __In-Memory Computation__ in Spark Website) so there will be no Physical Read/Write Operations from Mappers Instead there will be __Logical Read/Write__ Operations, Logica Read/Write is extremely Fast when compared to Physical Read/Write, because of this reason __Spark is Extremely Fast When compared to Hadoop MapReduce__.\n",
    "<br>\n",
    "__Note:__ Physical Read/Write -> Reading/Writing From/To HDD, Logical Read/Write -> Reading/Writing From/To RAM \n",
    "\n",
    "-  Spark uses __DAG(Directed Acyclic Graphs) Execution Engine__ that supports Acyclic Data Flow i.e DAG engine converts Users code into Series of Tasks and These tasks are DAG in nature i.e execution flow will be one task to other and never have cyclic relation between tasks. through this way Spark has clarity on what exactly is happening as a whole so that there is a scope of optimization, this is achieved using RDD(where RDD enables Spark to keeps track of all operations and transformations on the dataset). Where as in Hadoop it consumes the Jobs given by the user and Executes accordingly and gives the output, It doesn't have Big Picture of all tasks, so there is no scope of optimization. \n",
    "\n",
    "__3) Key Features that Make Spark Faster than Hadoop__\n",
    "-  In Memory Computing\n",
    "-  Spark Execution Engine(DAG)\n",
    "-  RDD's\n",
    "-  Spark Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hadoop MapReduce vs Spark__<br>\n",
    "How Node Failure is Handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hadoop MapReduce:__\n",
    "<img src = 'HadoopMapReduceWorkingExplained.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> All the Intermediate Outputs are stored in Hard disk i.e Out put produced by Node 1 is Stored in HardDisk and Node 6 Reades this output as input from HardDisk, if Node1 failed, then Hadoop has 2 other Nodes replicated with same Node1 data, so we can get the data from backup nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Spark:__\n",
    "<img src = 'SparkMapReduceWorkingExplained.PNG'/>\n",
    "<img src='SparkNodeFailureHandling.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> All the Intermediate outputs are stored In-Memory so Node 6 can Directly read output from Node 1, If Node1 Failed then How Spark is going to handle, as Spark doesnt have Replication nodes. Spark Makes use of RDD(Resilient Distributed Datasets) i.e every Spark operation produces a RDD i.e Spark keeps track of all the Operations(like Aggrigations etc..) and Transformations, so that we can get other Node X with Same as that of Node 1 from RDD, Spark Handles Fault Tolerance in this way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4)What is RDD?__\n",
    "-  RDD stands for Resilient Distributed Datasets, Resilient means Fault Tolerance\n",
    "-  Spark makes use of RDD to keep track of all the Operations and Transformations performed on Data set i.e it Creates RDD for every Operation or Transformation performed on Data Set.\n",
    "\n",
    "-  Below is the image that Explains with an Example\n",
    "<img src = 'RDD_Explained.PNG'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Example Illustrates a Case where if we want to check all the hdfs related errors from a text file, Then the Text file data is distributed to no.of blocks here its 5 then using filter() function we get Errors from the Log File then from that errors we are getting HDFS erros using filter() function, Lets say Partition 2 of Errors is down/lost due to Node 2 failure, then we can get Partition 2 of Error by simply running filter() function again on Partition 2 of LogFile, This is the Main use of RDD. \n",
    "-  __RDD__ is Core and __Heart__ of the Spark\n",
    "-  Spark Achieves __Fault Tolerance__ using RDD\n",
    "\n",
    "__Misconceptions on RDD's__\n",
    "-  All RDD's are  Stored in Memory: No It's not i.e if we want execute the above problem once again then it will execute as if it was executed for the first time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5) 2 Types of Functions in Spark__\n",
    "-  Transformation Functions: Which Transforms/modifies RDD's and they dont trigger Execution, In the Above Diagram textFile(), filter() are Transformation Functions\n",
    "-  Action Functions: Which triggers execution, In the above diagram count() is Action Function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
